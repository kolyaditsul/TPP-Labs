{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.Завантажити текст з вікіпедії та провести його обробку згідно п.1-3."
      ],
      "metadata": {
        "id": "7ej7kCR50JH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import re\n",
        "\n",
        "# Завантаження необхідних компонентів NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "text = \"Маши́нне навча́ння (МН, англ. machine learning, ML) — це галузь досліджень штучного інтелекту, зосереджена на розробці та вивченні статистичних алгоритмів.\"\n",
        "\n",
        "# Токенізація\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Фільтрація стоп-слів\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Стемінг\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Лематизація\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "print(\"Токени:\", tokens)\n",
        "print(\"Фільтровані токени:\", filtered_tokens)\n",
        "print(\"Стемінг:\", stemmed_tokens)\n",
        "print(\"Лематизація:\", lemmatized_tokens)\n",
        "\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"Частини мови:\", pos_tags)\n",
        "\n",
        "# Видалення спеціальних символів і чисел\n",
        "clean_text = re.sub(r'\\W|\\d', ' ', text)\n",
        "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "print(\"Очищений текст:\", clean_text)\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "count_vector = count_vectorizer.fit_transform([clean_text])\n",
        "print(\"Мішок слів (CountVectorizer):\", count_vector.toarray())\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vector = tfidf_vectorizer.fit_transform([clean_text])\n",
        "print(\"TFIDF Вектор:\", tfidf_vector.toarray())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5YhNiFAZlij",
        "outputId": "418111ac-fc72-4041-c5c5-b6746e03bdc8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токени: ['Маши́нне', 'навча́ння', '(', 'МН', ',', 'англ', '.', 'machine', 'learning', ',', 'ML', ')', '—', 'це', 'галузь', 'досліджень', 'штучного', 'інтелекту', ',', 'зосереджена', 'на', 'розробці', 'та', 'вивченні', 'статистичних', 'алгоритмів', '.']\n",
            "Фільтровані токени: ['Маши́нне', 'навча́ння', '(', 'МН', ',', 'англ', '.', 'machine', 'learning', ',', 'ML', ')', '—', 'це', 'галузь', 'досліджень', 'штучного', 'інтелекту', ',', 'зосереджена', 'на', 'розробці', 'та', 'вивченні', 'статистичних', 'алгоритмів', '.']\n",
            "Стемінг: ['маши́нне', 'навча́ння', '(', 'мн', ',', 'англ', '.', 'machin', 'learn', ',', 'ml', ')', '—', 'це', 'галузь', 'досліджень', 'штучного', 'інтелекту', ',', 'зосереджена', 'на', 'розробці', 'та', 'вивченні', 'статистичних', 'алгоритмів', '.']\n",
            "Лематизація: ['маши́нне', 'навча́ння', '(', 'мн', ',', 'англ', '.', 'machin', 'learn', ',', 'ml', ')', '—', 'це', 'галузь', 'досліджень', 'штучного', 'інтелекту', ',', 'зосереджена', 'на', 'розробці', 'та', 'вивченні', 'статистичних', 'алгоритмів', '.']\n",
            "Частини мови: [('Маши́нне', 'NN'), ('навча́ння', 'NNP'), ('(', '('), ('МН', 'NNP'), (',', ','), ('англ', 'NNP'), ('.', '.'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('ML', 'NNP'), (')', ')'), ('—', 'VBP'), ('це', 'JJ'), ('галузь', 'NNP'), ('досліджень', 'NNP'), ('штучного', 'NNP'), ('інтелекту', 'NNP'), (',', ','), ('зосереджена', 'NNP'), ('на', 'NNP'), ('розробці', 'NNP'), ('та', 'NNP'), ('вивченні', 'NNP'), ('статистичних', 'NNP'), ('алгоритмів', 'NNP'), ('.', '.')]\n",
            "Очищений текст: Маши нне навча ння МН англ machine learning ML це галузь досліджень штучного інтелекту зосереджена на розробці та вивченні статистичних алгоритмів\n",
            "Мішок слів (CountVectorizer): [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "TFIDF Вектор: [[0.21821789 0.21821789 0.21821789 0.21821789 0.21821789 0.21821789\n",
            "  0.21821789 0.21821789 0.21821789 0.21821789 0.21821789 0.21821789\n",
            "  0.21821789 0.21821789 0.21821789 0.21821789 0.21821789 0.21821789\n",
            "  0.21821789 0.21821789 0.21821789]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Завантажити ще один текст з вікіпедії на схожу тематику. Побудувати мішки слів для обох текстів. Перевірити їх схожість."
      ],
      "metadata": {
        "id": "wxI1wo4Rz8y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def fetch_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = ' '.join([para.text for para in paragraphs])\n",
        "    return article_text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Видалення посилань\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Видалення зайвих пробілів\n",
        "    text = re.sub(r'\\W', ' ', text)  # Видалення несловесних символів\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Приклад виклику функцій\n",
        "url = 'https://uk.wikipedia.org/wiki/Штучний_інтелект'\n",
        "text = fetch_text(url)\n",
        "tokens = clean_text(text)\n",
        "url_ml = 'https://uk.wikipedia.org/wiki/Машинне_навчання'\n",
        "text_ml = fetch_text(url_ml)\n",
        "tokens_ml = clean_text(text_ml)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Об'єднання токенів у один рядок для векторизації\n",
        "all_texts = [' '.join(tokens), ' '.join(tokens_ml)]\n",
        "vectorized_texts = vectorizer.fit_transform(all_texts)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity = cosine_similarity(vectorized_texts)\n",
        "print(f\"Косинусна схожість між текстом 'Штучний інтелект' і 'Машинне навчання': {similarity[0, 1]}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRZB-0pKt20T",
        "outputId": "dd31688c-a542-4253-8491-61a878faf91e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Косинусна схожість між текстом 'Штучний інтелект' і 'Машинне навчання': 0.6372928979425373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Завантажити з Kaggle датасет для навчання моделі на виявлення спаму"
      ],
      "metadata": {
        "id": "wIKQSpBs0VRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Завантаження та підготовка даних\n",
        "data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "data = data[['v1', 'v2']]\n",
        "data.columns = ['label', 'message']\n",
        "data['label'] = data['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Попередня обробка тексту\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "data['message'] = data['message'].apply(clean_text)\n",
        "\n",
        "# Векторизація тексту\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(data['message'])\n",
        "y = data['label']\n",
        "\n",
        "# Розділення даних на тренувальний та тестовий набори\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Навчання моделі\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Прогнозування та оцінка моделі\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Точність:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Звіт про класифікацію:\\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmS_HnWvwEMv",
        "outputId": "3843717b-716d-42dc-ac4b-4097a2858b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точність: 0.9802690582959641\n",
            "Звіт про класифікацію:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       949\n",
            "           1       0.99      0.87      0.93       166\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Підготувати з власних електронних листів вибірку для визначення спамуз використанням моделі з п.4 даного завдання. Зробити висновок щодо ефективності побудованої моделі."
      ],
      "metadata": {
        "id": "rhAd3IRswF3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# Переконайтеся, що ці ресурси NLTK завантажені\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Видалення тексту в дужках\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Заміна кількох пробілів на один\n",
        "    text = re.sub(r'\\W', ' ', text)  # Видалення всіх небуквених символів\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Завантаження даних\n",
        "data = pd.read_csv('email.csv')\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X = vectorizer.fit_transform(data['Message'])\n",
        "y = data['Category']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqOrFg87DBQR",
        "outputId": "7482ff57-6c0e-4c26-fcff-add29a26e70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9829596412556054\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       958\n",
            "        spam       1.00      0.88      0.94       157\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.99      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n",
            "Confusion Matrix:\n",
            "[[958   0]\n",
            " [ 19 138]]\n"
          ]
        }
      ]
    }
  ]
}